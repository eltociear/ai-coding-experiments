File	Prompt	NumPromptsToSuccess	notes
2_pubmed.py	create python code to create an abstract base class called AbstractPublication to represent publications, with a method called from_pubmed(). Then create a class called Article that inherits AbstractPublication. The from_pubmed() method should take in a pubmed record downloaded using the biopython.Entrez tools, and should convert the author list to a variable called authors and the title to a variable called title. In the main function, perform a pubmed search for the query "cognitive control" and convert each record into an instance of the Publication class, saving them to a dictionary that uses the pubmed ID as the key.	1	
3_LDA.py	Create python code to download 1000 abstracts from pubmed matching the query "cognitive control", clean up the text by removing standard English stopwords and lexicalizing the words, and apply topic modeling using latent Dirichlet allocation to find 10 topics. Print the highest scoring words for each topic.	1	
5_lmm.py	create a python application takes in a set of csv files, each of which includes two columns: one called "rt" that contains a response time in miiliseconds, and another "correct" that is binary (1/0) denoting whether the response was correct or not. there should also be another variable, called "compatible", that contains binary values (0/1) with half of the rows set to zero and half to 1. combine these files into a single data frame, using the individual file names as an index variable. then perform a linear mixed model that computes the effect of the compatible factor, with a random intercept and slope for the different file names. also create a test that automatically generates a set of data files with random values, and then tests the function to assess whether it properly returns the true values.	1	
7_download_github_code.py	Create python code that uses the github api to download the 100 most recently committed python files.	1	a version of this was used for all of the github analyses
10_mk_classification_data.py	please create code to generate some test data for this code (#9)	1	
15_DAG.py	 generate a python class to represent a directed graph. generate a function that takes in a directed graph and a two sets of vertices and and returns whether those two sets are d-separated. add a test that compares the results from this code to the d-separation results computed using the NetworkX library	1	
20_imgsmooth.py	create a python class to perform spatial smoothing on a nifti image using a median filter.# please create a test image for use in the example# please add code to present a plot of one slice from the input file next to one slice from the output file	1	
25_overfitting.py	create python code to simulate the effect of overfitting in regression. 1) generate synthetic data with 32 observations for two variables from a bivariate normal distribution with a correlation of 0.5.  2) fit three models to the data: a linear regression model, a 2nd-order polynomial regression model, and a 9-th order polynomial model.  3) Compute the error for each of these models on the synthetic training data, and on a synthetic test dataset generated from the same distribution.  4) plot the fitted lines for each of the fitted models overlaid on the training data.	1	
28_ascii_art.py	generate python code to read in an image and render the image using ascii art.  	1	
30_PCA_poweriteration.py	generate a class in the scikit-learn style to perform principal component analysis using the power iteration method.  the algorithm should be coded from scratch rather than using an external library.# create an additional function to compare the results of this method to those of the standard PCA function in scikit-learn	1	
31_ttest_wrapper.py	create a python wrapper for the statsmodels ttest_ind function, which takes the same arguments and returns a report similar to the one returned by the t.test function in R	1	
32_randomization.py	create a simulation to demonstrate the use of randomization to obtain a null distribution for hypothesis testing. First, generate a bivariate dataset with a sample size of 100 and a correlation of 0.1 between the two variables. Compute the correlation coefficient and obtain a p-value against the null hypothesis of r=0. Then, randomly shuffle one of the variables 5000 times and compute the correlation each time. Compute an empirical p-value for the actual R value based on the null distribution.	1	
9_bad_cv.py	Generate a python script that performs crossvalidation with feature selection incorrectly performed outside of the crossvalidation loop	2	
16_CLT.py	 Create code in python to perform a simulation that repeatedly generates samples of random variates from 6 different distributions (normal, uniform, chi-squared, poisson, exponential, and beta) and computes the mean of each sample, saving it for later use. Then, generate a figure that shows the distribution of the means for each of the distributions.	2	
21_PC.py	# please create a python class that implements the PC causal inference algorithm. # please code the algorithm from scratch rather than using an existing library.# please recode the independence test using partial correlation.  # please create a synthetic test dataset for testing the usage# please create a function to generate a synthetic dataset with simple causal structure	2	
22_textanalysis.py	generate a python function that takes in a piece of English text and performs linguistic analysis on it, returning values for sentiment analysis (positive/negative) and for linguistic complexity.# please also include the L2SCA and coh-metrix measures of complexity.	2	couldn't actually get the coh-metrix values but was able to get L2SCA
23_twostep.py	create python code to analyze human performance on the Daw two-step reinforcement learning task.  the code should take in a data frame with human responses on each trial for each of the two steps along with the outcomes from each step.  it should return the indices of model-based and model-free behavior.# please create code to generate a synthetic dataset and test it using this function	3	
24_balanced_cv.py	 create a new crossvalidation class that implements balanced cross-validation for regression that follows the scikit-learn class structure for crossvalidation objects.  this requires finding a set of splits that vary minimally in their distributions, which can be assessed using an F statistic.  	3	
29_pdf_generation.py	create python code to take a document written in Markdown and render it as a PDF, with an image for the header.   please use argparse for the input values	3	output not perfect, but it worked
1_linear_regression.py 	Create python code that generates a new class called LinearRegressionStats which extends sklearn.linear_model.LinearRegression() to compute the t statistic and p-value for each regressor in the model.  - first try used a dataset no longer included in sklearn - second try fixed that problem but generated a runtime error: RuntimeError: scikit-learn estimators should always specify their parameters in the signature of their __init__ (no varargs). <class '__main__.LinearRegressionStats'> with constructor (self, *args, **kwargs) doesn't  follow this convention. - third try gave error: __init__() got an unexpected keyword argument 'normalize' (that argument appears to have been deprecated according to chatgpt) - ran correctly on fourth try	4	
18_hurdle.py	generate a python class that fits hurdle regression model, with a scikit-learn type interface.	4	
6_pubmed_emails.py	 Create python code to find a set of articles matching the query "cognitive control" using pubmed.  For each article extract the first and last author and their affilations.  Then find their institutional web site and scrape their email address from the web site.  - on first try This code fails because raw_record[0] assumes that raw_record is not None.- still failed on second try due to logic- failed on third try because it assumed that record was a list eather than a dict- failed on fourth try due to             last_affiliation = last_author.get("AffiliationInfo", [{}])[0].get("Affiliation", "")- failed on fifth try due to             print(f"Last Author: {last_author['LastName']} {last_author['ForeName']}")- this code is failing to find the emails for any of the researchers, because you are searching for a URL in the affiliation field, which doesn't exist. you instead need to search the web using the name and affiliation information, and then find email addresses on that page.- please modify this code to return all email addresses found on the page, rather than a single address.	7	
8_code_complexity.py	create python code to load each python file from a directory called "python_files", and for each file compute cyclomatic complexity, maintainability index, and halsted complexity metrics for each file using the radon package	8	a version of this was used for all of the github analyses
11_mk_animation.py	Write a python function that takes as input a textfile, then create an  animation that: [1] Displays X lines of the textfile at a time, with Y characters per  line.  If the text file has more than Y characters in a line, then move  to the next line.   [2] Scross down Z lines of the textfile every second, in a continuous  way. [3] Animate it in a new popup window. [4] Record the animation for R seconds, and save it as a .avi file.	?	unable to actually test due to system library issues
4_ddm.py	Generate python code to simulate a drift diffusion model of response times. Simulate 1000 trials from this model, and estimate the drift rate, boundary separation, and starting point parameters using the EZ-diffusion model.	> 4	hallucinated equations
27_PAweather.py	generate python code to obtain daily weather reports from palo alto, california for 1960 to 2000.  from these data, compute the maximum temperature in each month, and plot the timeseries of maximum monthly temperatures over that time period.# please read the api key from a file called "noaa_api_key.txt"	>1	API labels seem to be wrong, couldn't find the right ones anywhere
13_logisticmap.py	implement python code to simulate a logistic map, and create a plot of x versus r.	>2	unclear what was wrong with code, but plot was empty
17_ddm_collapsing_plot.py	create python code to simulate a drift diffusion model with collapsing bounds. # - please return the entire timeseries of diffusion steps from the function.	>2	wasn't returning full timeseries, gave up
12_transformer.py	create python code to implement a three-layer transformer model, and train that model using a small text corpus.	>4	couldn't figure out how to use the latest torchtext
14_GES.py	create python code to implement the greedy equivalence search (GES) algorithm, and code to generate test data that can be used to test the implementation.	>4	unclear what was wrong, persistent KeyError, gave up
19_corridor.py	create a simulation in python that generates a version of the "corridor of stability" plot by Felix Schonbrodt.  The plot should show how correlation estimates become less variable as the sample size increases.	>4	had the right idea but the plot was not correct - likely a prompting failure
26_factoranalysis.py	please create python code to 1) load data from https://raw.githubusercontent.com/IanEisenberg/Self_Regulation_Ontology/master/Data/Complete_02-16-2019/meaningful_variables.csv into a data frame, select all variables that begin with 'upps_impulsivity_survey', 'sensation_seeking_survey', 'bis11_survey', or 'dickman_survey', 3) perform exploratory factor analysis over these variables using a range of 1 to 5 factors, 4) identify which model is preferred based on minimum BIC, and 5) for each factor in the preferred solution, display which variables were more strongly loaded on that factor versus other factors. Please write this in a modular way and output one function at a time.	>5	couldn't find a way to compute BIC.  tried using R package to do FA but failed with rpy2 problem, gave up