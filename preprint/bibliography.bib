@Book{wickham2015,
 author = {Wickham, Hadley},
 title = {R Packages},
 year = {2015},
 isbn = {1491910593, 9781491910597},
 edition = {1st},
 publisher = {O'Reilly Media, Inc.},
}

@article{knuth1984,
  title={Literate programming},
  author={Knuth, Donald E.},
  journal={The Computer Journal},
  volume={27},
  number={2},
  pages={97--111},
  year={1984},
  publisher={British Computer Society}
}

@Misc{Bubeck2023,
  author     = {Bubeck, Sébastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and Nori, Harsha and Palangi, Hamid and Ribeiro, Marco Tulio and Zhang, Yi},
  month      = apr,
  note       = {arXiv:2303.12712 [cs]},
  title      = {Sparks of {Artificial} {General} {Intelligence}: {Early} experiments with {GPT}-4},
  year       = {2023},
  abstract   = {Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.},
  doi        = {10.48550/arXiv.2303.12712},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
  publisher  = {arXiv},
  shorttitle = {Sparks of {Artificial} {General} {Intelligence}},
  url        = {http://arxiv.org/abs/2303.12712},
  urldate    = {2023-04-24},
}

@Book{Fowler2019,
  author     = {Fowler, Martin},
  publisher  = {Addison-Wesley},
  title      = {Refactoring: {Improving} the {Design} of {Existing} {Code}},
  year       = {2019},
  isbn       = {9780134757599},
  note       = {Google-Books-ID: o69NtAEACAAJ},
  abstract   = {Martin Fowler's guide to reworking bad code into well-structured code Refactoring improves the design of existing code and enhances software maintainability, as well as making existing code easier to understand. Original Agile Manifesto signer and software development thought leader, Martin Fowler, provides a catalog of refactorings that explains why you should refactor; how to recognize code that needs refactoring; and how to actually do it successfully, no matter what language you use.  Refactoring principles: understand the process and general principles of refactoring Code smells: recognize "bad smells" in code that signal opportunities to refactor Application improvement: quickly apply useful refactorings to make a program easier to comprehend and change Building tests: writing good tests increases a programmer's effectiveness Moving features: an important part of refactoring is moving elements between contexts Data structures: a collection of refactorings to organize data, an important role in programs Conditional Logic: use refactorings to make conditional sections easier to understand APIs: modules and their functions are the building blocks of our software, and APIs are the joints that we use to plug them together Inheritance: it is both very useful and easy to misuse, and it's often hard to see the misuse until it's in the rear-view mirror---refactorings can fix the misuse  Examples are written in JavaScript, but you shouldn't find it difficult to adapt the refactorings to whatever language you are currently using as they look mostly the same in different languages. "Whenever you read [Refactoring], it's time to read it again. And if you haven't read it yet, please do before writing another line of code." -David Heinemeier Hansson, Creator of Ruby on Rails, Founder \& CTO at Basecamp "Any fool can write code that a computer can understand. Good programmers write code that humans can understand." -M. Fowler (1999)},
  keywords   = {Computers / Programming / Object Oriented, Computers / Software Development \& Engineering / General},
  language   = {en},
  shorttitle = {Refactoring},
}

@Article{Ji2023,
  author   = {Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Yejin and Dai, Wenliang and Madotto, Andrea and Fung, Pascale},
  journal  = {ACM Computing Surveys},
  title    = {Survey of {Hallucination} in {Natural} {Language} {Generation}},
  year     = {2023},
  issn     = {0360-0300, 1557-7341},
  month    = dec,
  note     = {arXiv:2202.03629 [cs]},
  number   = {12},
  pages    = {1--38},
  volume   = {55},
  abstract = {Natural Language Generation (NLG) has improved exponentially in recent years thanks to the development of sequence-to-sequence deep learning technologies such as Transformer-based language models. This advancement has led to more fluent and coherent NLG, leading to improved development in downstream tasks such as abstractive summarization, dialogue generation and data-to-text generation. However, it is also apparent that deep learning based generation is prone to hallucinate unintended text, which degrades the system performance and fails to meet user expectations in many real-world scenarios. To address this issue, many studies have been presented in measuring and mitigating hallucinated texts, but these have never been reviewed in a comprehensive manner before. In this survey, we thus provide a broad overview of the research progress and challenges in the hallucination problem in NLG. The survey is organized into two parts: (1) a general overview of metrics, mitigation methods, and future directions; and (2) an overview of task-specific research progress on hallucinations in the following downstream tasks, namely abstractive summarization, dialogue generation, generative question answering, data-to-text generation, machine translation, and visual-language generation. This survey serves to facilitate collaborative efforts among researchers in tackling the challenge of hallucinated texts in NLG.},
  doi      = {10.1145/3571730},
  keywords = {A.1, Computer Science - Computation and Language},
  url      = {http://arxiv.org/abs/2202.03629},
  urldate  = {2023-04-24},
}

@Article{Wagenmakers2007,
  author   = {Wagenmakers, Eric-Jan and Van Der Maas, Han L. J. and Grasman, Raoul P. P. P.},
  journal  = {Psychonomic Bulletin \& Review},
  title    = {An {EZ}-diffusion model for response time and accuracy},
  year     = {2007},
  issn     = {1531-5320},
  month    = feb,
  number   = {1},
  pages    = {3--22},
  volume   = {14},
  abstract = {The EZ-diffusion model for two-choice response time tasks takes mean response time, the variance of response time, and response accuracy as inputs. The model transforms these data via three simple equations to produce unique values for the quality of information, response conservativeness, and nondecision time. This transformation of observed data in terms of unobserved variables addresses the speed—accuracy trade-off and allows an unambiguous quantification of performance differences in two-choice response time tasks. The EZ-diffusion model can be applied to data-sparse situations to facilitate individual subject analysis. We studied the performance of the EZ-diffusion model in terms of parameter recovery and robustness against misspecification by using Monte Carlo simulations. The EZ model was also applied to a real-world data set.},
  doi      = {10.3758/BF03194023},
  keywords = {Boundary Separation, Diffusion Model, Drift Rate, Error Response, Parameter Recovery},
  language = {en},
  url      = {https://doi.org/10.3758/BF03194023},
  urldate  = {2023-04-24},
}

@InProceedings{FinnieAnsley2022,
  author     = {Finnie-Ansley, James and Denny, Paul and Becker, Brett A. and Luxton-Reilly, Andrew and Prather, James},
  booktitle  = {Australasian {Computing} {Education} {Conference}},
  title      = {The robots are coming: {Exploring} the implications of openai codex on introductory programming},
  year       = {2022},
  pages      = {10--19},
  shorttitle = {The robots are coming},
}

@Book{Halstead1977,
  author    = {Halstead, Maurice Howard},
  publisher = {Elsevier},
  title     = {Elements of {Software} {Science}},
  year      = {1977},
  isbn      = {9780444002150},
  note      = {Google-Books-ID: rRIpAQAAMAAJ},
  language  = {en},
}

@Misc{Chen2021,
  author    = {Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and Ray, Alex and Puri, Raul and Krueger, Gretchen and Petrov, Michael and Khlaaf, Heidy and Sastry, Girish and Mishkin, Pamela and Chan, Brooke and Gray, Scott and Ryder, Nick and Pavlov, Mikhail and Power, Alethea and Kaiser, Lukasz and Bavarian, Mohammad and Winter, Clemens and Tillet, Philippe and Such, Felipe Petroski and Cummings, Dave and Plappert, Matthias and Chantzis, Fotios and Barnes, Elizabeth and Herbert-Voss, Ariel and Guss, William Hebgen and Nichol, Alex and Paino, Alex and Tezak, Nikolas and Tang, Jie and Babuschkin, Igor and Balaji, Suchir and Jain, Shantanu and Saunders, William and Hesse, Christopher and Carr, Andrew N. and Leike, Jan and Achiam, Josh and Misra, Vedant and Morikawa, Evan and Radford, Alec and Knight, Matthew and Brundage, Miles and Murati, Mira and Mayer, Katie and Welinder, Peter and McGrew, Bob and Amodei, Dario and McCandlish, Sam and Sutskever, Ilya and Zaremba, Wojciech},
  month     = jul,
  note      = {arXiv:2107.03374 [cs]},
  title     = {Evaluating {Large} {Language} {Models} {Trained} on {Code}},
  year      = {2021},
  abstract  = {We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8\% of the problems, while GPT-3 solves 0\% and GPT-J solves 11.4\%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2\% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.},
  doi       = {10.48550/arXiv.2107.03374},
  keywords  = {Computer Science - Machine Learning},
  publisher = {arXiv},
  url       = {http://arxiv.org/abs/2107.03374},
  urldate   = {2023-04-24},
}

@Misc{Prystawski2022,
  author    = {Prystawski, Ben and Thibodeau, Paul and Goodman, Noah},
  month     = sep,
  note      = {arXiv:2209.08141 [cs]},
  title     = {Psychologically-informed chain-of-thought prompts for metaphor understanding in large language models},
  year      = {2022},
  abstract  = {Probabilistic models of language understanding are interpretable and structured, for instance models of metaphor understanding describe inference about latent topics and features. However, these models are manually designed for a specific task. Large language models (LLMs) can perform many tasks through in-context learning, but they lack the clear structure of probabilistic models. In this paper, we use chain-of-thought prompts to introduce structures from probabilistic models into LLMs. These prompts lead the model to infer latent variables and reason about their relationships to choose appropriate paraphrases for metaphors. The latent variables and relationships chosen are informed by theories of metaphor understanding from cognitive psychology. We apply these prompts to the two largest versions of GPT-3 and show that they can improve paraphrase selection.},
  doi       = {10.48550/arXiv.2209.08141},
  keywords  = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
  publisher = {arXiv},
  url       = {http://arxiv.org/abs/2209.08141},
  urldate   = {2023-04-25},
}

@Misc{Wei2023,
  author    = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
  month     = jan,
  note      = {arXiv:2201.11903 [cs]},
  title     = {Chain-of-{Thought} {Prompting} {Elicits} {Reasoning} in {Large} {Language} {Models}},
  year      = {2023},
  abstract  = {We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
  doi       = {10.48550/arXiv.2201.11903},
  keywords  = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
  publisher = {arXiv},
  url       = {http://arxiv.org/abs/2201.11903},
  urldate   = {2023-04-25},
}

@Comment{jabref-meta: databaseType:bibtex;}
